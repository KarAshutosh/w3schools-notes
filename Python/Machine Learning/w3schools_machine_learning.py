# -*- coding: utf-8 -*-
"""w3schools_machine_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y0lbEPV0z8p_xe6Xnij3yLwu7dqCtxpE

# Machine Learning notes

W3schools' machine learning notes
by Ashutosh Karanam

Src: https://www.w3schools.com/python/python_ml_getting_started.asp

Date: 17th April, 2024

---------------------------

## Setup Dependencies to Run Code

We will be using NumPy, SciPy and other modules. Import at once here or import perticular modules later
"""

import numpy as np
from scipy import stats
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn import linear_model
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn import datasets
from sklearn.model_selection import ShuffleSplit, LeavePOut, LeaveOneOut, StratifiedKFold, KFold, cross_val_score
from sklearn.cluster import KMeans
import pandas as pd
from sklearn.tree import plot_tree

"""## ----------------------------------------------------------------------------

## Getting Started

Getting Started talks about what a `Data Set` and `Data Types`

### Data Set
Data set is any collection of data. It can be anything from an array to a complete database.

### Data Types
Type of data we are dealing with. Has three main categories:
* **Numerical**: numbers
  * **Discrete Data**: limited to integers
  * **Continuous Data**: can be any number
* **Categorical**: values that cannot be measured up against each other. Example: a color value, or any yes/no values.
* **Ordinal**: like categorical data, but can be measured up against each other. Example: school grades where A is better than B and so on

## ----------------------------------------------------------------------------

## Mean Median Mode
Explains how to find mean, median and mode using NumPy and SciPy

Uses `np.median(speed)`, `np.median(speed)` and `scipy.stats.mode(speed)`

### Import Dependencies

(If not done so already)
"""

import numpy as np
from scipy import stats

"""In the following example, we have registered the speed of 13 cars:
```
speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]
```

We will be performing actions on this data set

### Mean: Use the NumPy mean() method to find the average speed:

`(99+86+87+88+111+86+103+87+94+78+77+85+86) / 13 = 89.77`
"""

speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]

x = np.mean(speed)

print(x)

"""### Median: Use the NumPy median() method to find the middle value:

77, 78, 85, 86, 86, 86, **87**, 87, 88, 94, 99, 103, 111

"""

speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]

x = np.median(speed)

print(x)

"""Note: It is important that the numbers are sorted before you can find the median.

### Mode: Use the SciPy mode() method to find the number that appears the most

99, **86**, 87, 88, 111, **86**, 103, 87, 94, 78, 77, 85, **86**
"""

speed = [99,86,87,88,111,86,103,87,94,78,77,85,86]

x = stats.mode(speed)

print(x)

"""## ----------------------------------------------------------------------------

## Standard Deviation

Explains how to find the standard deviation and variance using NumPy

Uses `np.std(speed)` and `np.var(speed)`

Standard Deviation:
Describes how spread out the values are

Variance: square of standard deviation

### Import Dependencies

(If not done so already)
"""

import numpy as np

"""In the following example, we have

registered the speed of 7 cars:
```
speed = [32,111,138,28,59,77,97]
```

We will be performing actions on this data set

### Standard deviation: Use the NumPy std() method to find the standard deviation:
"""

speed = [32,111,138,28,59,77,97]

x = np.std(speed)

print(x)

"""### Variance: Use the NumPy var() method to find the variance:




"""

speed = [32,111,138,28,59,77,97]

x = np.var(speed)

print(x)

"""## ----------------------------------------------------------------------------

## Percentile

Explains how to find percentile

Uses `np.percentile(ages, 75)` to find the `.75th` percentile

### Import Dependencies

(If not done so already)
"""

import numpy as np

"""In the following example, we have registered the ages of 21 people:
```
ages = [5,31,43,48,50,41,7,11,15,39,80,82,32,2,8,6,25,36,27,61,31]
```

We will be performing actions on this data set

### Percentile: Use the NumPy percentile() method to find the percentiles:

75th percentile
"""

ages = [5,31,43,48,50,41,7,11,15,39,80,82,32,2,8,6,25,36,27,61,31]

x = np.percentile(ages, 75)

print(x)

"""90th percentile"""

ages = [5,31,43,48,50,41,7,11,15,39,80,82,32,2,8,6,25,36,27,61,31]

x = np.percentile(ages, 90)

print(x)

"""## ----------------------------------------------------------------------------

## Data Distribution

Explains creating and plotting big data sets

Uses `np.random.uniform(0.0, 5.0, 250)` to create large data set

Uses `plt.hist(x, 5)` and `plt.show()` to plot the data set

### Import Dependencies

(If not done so already)
"""

import numpy as np
import matplotlib.pyplot as plt

"""### Create an array containing 250 random floats between 0 and 5


"""

x = np.random.uniform(0.0, 5.0, 250)

print(x)

"""### Draw a histogram with 5 bars"""

x = np.random.uniform(0.0, 5.0, 250)

plt.hist(x, 5)
plt.show()

"""## ----------------------------------------------------------------------------

## Normal Data Distribution

Explains how a typical normal distribution looks

Uses `np.random.normal(5.0, 1.0, 100000)`to create large data set

Uses plt.hist(x, 100) and plt.show() to plot the dataset

### Import Dependencies

(If not done so already)
"""

import numpy as np
import matplotlib.pyplot as plt

"""### Plotting a typical normal data distribution with mean 5 and varience 1


"""

x = np.random.normal(5.0, 1.0, 100000)

plt.hist(x, 100)
plt.show()

"""## ----------------------------------------------------------------------------

## Scatter Plot
Explains how to plot a scatter plot

Uses plt.scatter(x, y) and plt.show() to plot the dataset

A scatter plot is a diagram where each value in the data set is represented by a dot.

### Import Dependencies

(If not done so already)
"""

import matplotlib.pyplot as plt

"""In the following example, we have:
The x array represents the age of each car.
The y array represents the speed of each car.
```
x = [5,7,8,7,2,17,2,9,4,11,12,9,6]

y = [99,86,87,88,111,86,103,87,94,78,77,85,86]
```

We will be performing actions on this data set

### Use the scatter() method to draw a scatter plot diagram:
"""

x = [5,7,8,7,2,17,2,9,4,11,12,9,6]
y = [99,86,87,88,111,86,103,87,94,78,77,85,86]

plt.scatter(x, y)
plt.show()

"""## ----------------------------------------------------------------------------

## Linear Regression

Linear regression uses the relationship between the data-points to draw a straight line through all them.

Uses `slope, intercept, r, p, std_err = stats.linregress(x, y)`

### Import Dependencies

(If not done so already)
"""

import matplotlib.pyplot as plt
from scipy import stats

"""In the following example, we have:
The x array represents the age of each car.
The y array represents the speed of each car.
```
x = [5,7,8,7,2,17,2,9,4,11,12,9,6]

y = [99,86,87,88,111,86,103,87,94,78,77,85,86]
```

We will be performing actions on this data set

### Draw the line of Linear Regression
"""

x = [5,7,8,7,2,17,2,9,4,11,12,9,6]
y = [99,86,87,88,111,86,103,87,94,78,77,85,86]

slope, intercept, r, p, std_err = stats.linregress(x, y)

def myfunc(x):
  return slope * x + intercept

mymodel = list(map(myfunc, x))


plt.scatter(x, y)
plt.plot(x, mymodel)
plt.show()

"""### Explaining the previous code

Execute a method that returns some important key values of Linear Regression which include slope, intercept, rvalue, pvalue and stderror:

```
slope, intercept, r, p, std_err = stats.linregress(x, y)
```

What they are:
* `r-value` measures how good the fit is
* `p-value` is a number describing how likely it is that your data would have occurred under the null hypothesis of your statistical test
* `std_err` the measurement of how dispersed a sample's means are from the population mean

Note: The p-value helps decide whether to accept or reject the null hypothesis. It measures the probability there is no relationship between variables. A low p-value gives evidence against the null hypothesis.

Create a function that uses the slope and intercept values to return a new value. This new value represents where on the y-axis the corresponding x value will be placed:

```
def myfunc(x):
  return slope * x + intercept
```

Run each value of the x array through the function. This will result in a new array with new values for the y-axis:

```
mymodel = list(map(myfunc, x))
```

Draw the original scatter plot:

```
plt.scatter(x, y)
```

Draw the line of linear regression:

```
plt.plot(x, mymodel)
```

Display the diagram:
```
plt.show()
```

### Coefficient of correlation (r) to check how well does the data fits in a linear regression

The `r` value ranges from -1 to 1, where 0 means no relationship, and 1 (and -1) means 100% related.
"""

x = [5,7,8,7,2,17,2,9,4,11,12,9,6]
y = [99,86,87,88,111,86,103,87,94,78,77,85,86]

slope, intercept, r, p, std_err = stats.linregress(x, y)

print(r)

"""### Predict Future Values: Predict the speed of a 10 years old car

"""

x = [5,7,8,7,2,17,2,9,4,11,12,9,6]
y = [99,86,87,88,111,86,103,87,94,78,77,85,86]

slope, intercept, r, p, std_err = stats.linregress(x, y)

def myfunc(x):
  return slope * x + intercept

speed = myfunc(10)

print(speed)

"""### Bad Fit: These values for the x- and y-axis should result in a very bad fit for linear regression:

Linear regression is not always the best way to predict values. Depending on the data set, different methods must be used
"""

x = [89,43,36,36,95,10,66,34,38,20,26,29,48,64,6,5,36,66,72,40]
y = [21,46,3,35,67,95,53,72,58,10,26,34,90,33,38,20,56,2,47,15]

slope, intercept, r, p, std_err = stats.linregress(x, y)

def myfunc(x):
  return slope * x + intercept

mymodel = list(map(myfunc, x))

print(r)

plt.scatter(x, y)
plt.plot(x, mymodel)
plt.show()

"""## ----------------------------------------------------------------------------

## Polynomial Regression

Polynomial regression, like linear regression, uses the relationship between the variables x and y to find the best way to draw a line through the data points.

Uses `mymodel = np.poly1d(numpy.polyfit(x, y, 3))` to make the model and `r2_score` from `sklearn.metrics` to calculate r-value

### Import Dependencies

(If not done so already)
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score

"""In the following example, we have an `x` and `y` array
```
x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]

y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]
```

We will be performing actions on this data set

### Draw the line of Polynomial Regression:
"""

x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]
y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]

mymodel = np.poly1d(np.polyfit(x, y, 3))

myline = np.linspace(1, 22, 100)

print("---------------------------------------------------------------------")
print(np.polyfit(x, y, 3))
print("---------------------------------------------------------------------")
print(mymodel)
print("---------------------------------------------------------------------")
print(r2_score(y, mymodel(x)))
print("---------------------------------------------------------------------")

plt.scatter(x, y)
plt.plot(myline, mymodel(myline))
plt.show()

"""### Explaining the previous code

NumPy has a method that lets us make a polynomial model

NumPy's `polyfit` function performs polynomial regression of degree 3 (cubic polynomial) on the data points (x, y). This means NumPy's polyfit function fits a polynomial of specified degree to a set of data points. It calculates the coefficients of the polynomial that best fits the data in a least squares sense. The resulting polynomial function is then stored in the variable `mymodel` as a `poly1d` object

A poly1d object represents a polynomial function of one variable (the `ax^n + bx^(n-1)+ ... + k` form). It's a convenient way to work with polynomial functions

```
mymodel = np.poly1d(numpy.polyfit(x, y, 3))
```

Note:  You can create a poly1d object in various ways:
* Using numpy.poly1d(coeffs): Pass the coefficients of the polynomial to create a poly1d object.
* Using polynomial fitting functions like numpy.polyfit, which returns a poly1d object representing the fitted polynomial.

Then specify how the line will display, we start at position 1, and end at position 22.

We generate 100 evenly spaced points between 1 and 22 (inclusive) using NumPy's linspace function. These points will be used to plot the fitted polynomial curve.


```
myline = np.linspace(1, 22, 100)
```

Python and the Sklearn module will compute the r-squared value for you, all you have to do is feed it with the x and y arrays:


```
print(r2_score(y, mymodel(x)))
```

### Predicting future values
"""

x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]
y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]

mymodel = np.poly1d(np.polyfit(x, y, 3))

speed = mymodel(17)
print(speed)

"""### Bad Fit

"""

x = [89,43,36,36,95,10,66,34,38,20,26,29,48,64,6,5,36,66,72,40]
y = [21,46,3,35,67,95,53,72,58,10,26,34,90,33,38,20,56,2,47,15]

mymodel = np.poly1d(np.polyfit(x, y, 3))

myline = np.linspace(2, 95, 100)

print(r2_score(y, mymodel(x)))

plt.scatter(x, y)
plt.plot(myline, mymodel(myline))
plt.show()

"""## ----------------------------------------------------------------------------

## Multiple Regression

Performs linear regression on a csv using `regr = linear_model.LinearRegression()`

Also predicts using `regr.predict([[2300, 1300]])` and finds coefficients using `regr.coef_`

### Import Dependencies

(If not done so already)
"""

import pandas
from sklearn import linear_model

"""In the following example, we will use a csv file called data.csv

Download the csv file below

### Download csv file
"""

import io
import os

data = """
Car,Model,Volume,Weight,CO2
Toyoty,Aygo,1000,790,99
Mitsubishi,Space Star,1200,1160,95
Skoda,Citigo,1000,929,95
Fiat,500,900,865,90
Mini,Cooper,1500,1140,105
VW,Up!,1000,929,105
Skoda,Fabia,1400,1109,90
Mercedes,A-Class,1500,1365,92
Ford,Fiesta,1500,1112,98
Audi,A1,1600,1150,99
Hyundai,I20,1100,980,99
Suzuki,Swift,1300,990,101
Ford,Fiesta,1000,1112,99
Honda,Civic,1600,1252,94
Hundai,I30,1600,1326,97
Opel,Astra,1600,1330,97
BMW,1,1600,1365,99
Mazda,3,2200,1280,104
Skoda,Rapid,1600,1119,104
Ford,Focus,2000,1328,105
Ford,Mondeo,1600,1584,94
Opel,Insignia,2000,1428,99
Mercedes,C-Class,2100,1365,99
Skoda,Octavia,1600,1415,99
Volvo,S60,2000,1415,99
Mercedes,CLA,1500,1465,102
Audi,A4,2000,1490,104
Audi,A6,2000,1725,114
Volvo,V70,1600,1523,109
BMW,5,2000,1705,114
Mercedes,E-Class,2100,1605,115
Volvo,XC70,2000,1746,117
Ford,B-Max,1600,1235,104
BMW,216,1600,1390,108
Opel,Zafira,1600,1405,109
Mercedes,SLK,2500,1395,120
"""

if os.path.exists('data.csv'):
    os.remove('data.csv')

# Convert the raw data to a DataFrame
df = pd.read_csv(io.StringIO(data))

# Write DataFrame to a CSV file
df.to_csv('data.csv', index=False)

print("CSV file saved in Google Drive.")

"""Link to data.csv file

Official w3schools:
https://www.w3schools.com/python/data.csv

Alternate:
https://www.w3schools.com/python/data.csv

### Linear Regression model on csv file
"""

df = pd.read_csv("data.csv")

X = df[['Weight', 'Volume']]
y = df['CO2']

regr = linear_model.LinearRegression()
regr.fit(X, y)

# Predict the CO2 emission of a car where:
## weight is 2300kg,
## volume is 1300cm3
predictedCO2 = regr.predict([[2300, 1300]])

print("Coefficient values of the regression object")
print(regr.coef_)
print("CO2 emission prediction")
print(predictedCO2)

"""### Explaining the code above

Reading data:

```
df = pandas.read_csv("data.csv")
```

Making a list of the independent values and call this variable `X`.

Putting the dependent values in a variable called `y`.



```
X = df[['Weight', 'Volume']]
y = df['CO2']
```

The `LinearRegression()` method to create a linear regression object.

This object has a method called `fit()` that takes the independent and dependent values as parameters and fills the regression object with data that describes the relationship

```
regr = linear_model.LinearRegression()
regr.fit(X, y)
```

Now we have a regression object that are ready to predict CO2 values based on a car's weight and volume:

```
predictedCO2 = regr.predict([[2300, 1300]])
```

We can ask for the coefficient value of weight against CO2, and for volume against CO2.

```
print(regr.coef_)
```

Note: The coefficient is a factor that describes the relationship with an unknown variable. Basically, in `ax + by`, `a` and `b` are coefficients

## ----------------------------------------------------------------------------

## Scale

We can scale data into new values that are easier to compare when, say, different units are used

.


Here, `scaledX = scale.fit_transform(X)` is used where `scale = StandardScaler()` to fit `X`

Even new data point must be scaled in same way as training data which can be done using `scaled = scale.transform([[2300, 1.3]])`

### Import Dependencies

(If not done so already)
"""

import pandas
from sklearn import linear_model
from sklearn.preprocessing import StandardScaler

"""In the following example, we will use a csv file called data.csv

Download the csv file below

### Download csv file
"""

import io
import os

data = """
Car,Model,Volume,Weight,CO2
Toyoty,Aygo,1.0,790,99
Mitsubishi,Space Star,1.2,1160,95
Skoda,Citigo,1.0,929,95
Fiat,500,0.9,865,90
Mini,Cooper,1.5,1140,105
VW,Up!,1.0,929,105
Skoda,Fabia,1.4,1109,90
Mercedes,A-Class,1.5,1365,92
Ford,Fiesta,1.5,1112,98
Audi,A1,1.6,1150,99
Hyundai,I20,1.1,980,99
Suzuki,Swift,1.3,990,101
Ford,Fiesta,1.0,1112,99
Honda,Civic,1.6,1252,94
Hundai,I30,1.6,1326,97
Opel,Astra,1.6,1330,97
BMW,1,1.6,1365,99
Mazda,3,2.2,1280,104
Skoda,Rapid,1.6,1119,104
Ford,Focus,2.0,1328,105
Ford,Mondeo,1.6,1584,94
Opel,Insignia,2.0,1428,99
Mercedes,C-Class,2.1,1365,99
Skoda,Octavia,1.6,1415,99
Volvo,S60,2.0,1415,99
Mercedes,CLA,1.5,1465,102
Audi,A4,2.0,1490,104
Audi,A6,2.0,1725,114
Volvo,V70,1.6,1523,109
BMW,5,2.0,1705,114
Mercedes,E-Class,2.1,1605,115
Volvo,XC70,2.0,1746,117
Ford,B-Max,1.6,1235,104
BMW,216,1.6,1390,108
Opel,Zafira,1.6,1405,109
Mercedes,SLK,2.5,1395,120
"""

if os.path.exists('data.csv'):
    os.remove('data.csv')

# Convert the raw data to a DataFrame
df = pd.read_csv(io.StringIO(data))

# Write DataFrame to a CSV file
df.to_csv('data.csv', index=False)

print("CSV file saved in Google Drive.")

"""Link to data.csv file

Official w3schools:
https://www.w3schools.com/python/data.csv

Alternate:
https://www.w3schools.com/python/data.csv

### Scaling `Weight` and `Volume`
"""

scale = StandardScaler()

df = pandas.read_csv("data.csv")

X = df[['Weight', 'Volume']]

scaledX = scale.fit_transform(X)

print(scaledX)

"""### Predicting with scaled `Weight` and `Volume`"""

scale = StandardScaler()

df = pandas.read_csv("data.csv")

X = df[['Weight', 'Volume']]
y = df['CO2']

scaledX = scale.fit_transform(X)

regr = linear_model.LinearRegression()
regr.fit(scaledX, y)

scaled = scale.transform([[2300, 1.3]])

predictedCO2 = regr.predict([scaled[0]])
print(predictedCO2)

"""### Explaining the above code

StandardScaler is used to standardize features by removing the mean and scaling to unit variance.

```
scale = StandardScaler()
```

This line scales the input features X using the fit_transform method of the scale object, which standardizes the features by:

* removing the mean and
* scaling to unit variance

```
scaledX = scale.fit_transform(X)
```

This line scales a new data point, [2300, 1.3], using the transform method of the scale object

This is necessary to ensure that the new data point is scaled in the same way as the training data

```
scaled = scale.transform([[2300, 1.3]])
```

### Theory

The standardization method uses this formula:

`z = (x - u) / s`

Where `z` is the new value, `x` is the original value, `u` is the mean and `s` is the standard deviation.

## ----------------------------------------------------------------------------

## Train/Test

80% for training, and 20% for testing

Split using simple array stuff like:
* `train_x = x[:80]` and `test_x = x[80:]`
* `train_y = y[:80]` and `test_y = y[80:]`

### Import Dependencies

(If not done so already)
"""

import numpy as np
import matplotlib.pyplot as plt

"""In this case we would like to measure the relationship between the minutes a customer stays in the shop and how much money they spend.

### Example training code
"""

np.random.seed(2)

x = np.random.normal(3, 1, 100)
y = np.random.normal(150, 40, 100) / x

train_x = x[:80]
train_y = y[:80]

test_x = x[80:]
test_y = y[80:]

mymodel = np.poly1d(np.polyfit(train_x, train_y, 4))

myline = np.linspace(0, 6, 100)

r2 = r2_score(train_y, mymodel(train_x))

print(r2)

plt.scatter(train_x, train_y)
plt.plot(myline, mymodel(myline))
plt.show()

"""### Example testing code

"""

np.random.seed(2)

x = np.random.normal(3, 1, 100)
y = np.random.normal(150, 40, 100) / x

train_x = x[:80]
train_y = y[:80]

test_x = x[80:]
test_y = y[80:]

mymodel = np.poly1d(np.polyfit(train_x, train_y, 4))

r2 = r2_score(test_y, mymodel(test_x))

print(r2)

# How much money will a buying customer spend, if she or he stays in the shop for 5 minutes?

print(mymodel(5))

"""## ----------------------------------------------------------------------------

## Decision Tree

Make decision tree data numerical using pandas `map()` method

Explains `gini`

Uses `DecisionTreeClassifier()` to make the model

### Theory

Decision tree data must be numeric. Use pandas `map()` method to map string to numerical value

**Setting up data to work in decision tree**

To make a decision tree, all data has to be numerical.

We have to convert the non numerical columns into numerical values.

This can be done through pandas `map()` method

Example:
```
d = {'UK': 0, 'USA': 1, 'N': 2}
df['Nationality'] = df['Nationality'].map(d)
d = {'YES': 1, 'NO': 0}
df['Go'] = df['Go'].map(d)

print(df)
```

**The Gini method (aspect of decision tree):**

`gini`: refers to the quality of the split, and is always a number between 0.0 and 0.5, where 0.0 would mean all of the samples got the same result, and 0.5 would mean that the split is done exactly in the middle.

.

The Gini method uses this formula:

`Gini = 1 - (x/n)^2 - (y/n)^2`

Where `x` is the number of positive answers("GO"), `n` is the number of samples, and `y` is the number of negative answers ("NO"), which gives us this calculation

### Import Dependencies

(If not done so already)
"""

import pandas as pd
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt

"""We will be performing actions on a CSV file



```
Age,Experience,Rank,Nationality,Go
36,10,9,UK,NO
42,12,4,USA,NO
23,4,6,N,NO
52,4,4,USA,NO
43,21,8,USA,YES
44,14,5,UK,NO
66,3,7,N,YES
35,14,9,UK,YES
52,13,7,N,YES
35,5,9,N,YES
24,3,5,USA,NO
18,3,7,UK,YES
45,9,9,UK,YES
```

### Download csv file
"""

import io
import os

data = """
Age,Experience,Rank,Nationality,Go
36,10,9,UK,NO
42,12,4,USA,NO
23,4,6,N,NO
52,4,4,USA,NO
43,21,8,USA,YES
44,14,5,UK,NO
66,3,7,N,YES
35,14,9,UK,YES
52,13,7,N,YES
35,5,9,N,YES
24,3,5,USA,NO
18,3,7,UK,YES
45,9,9,UK,YES
"""

if os.path.exists('data.csv'):
    os.remove('data.csv')

# Convert the raw data to a DataFrame
df = pd.read_csv(io.StringIO(data))

# Write DataFrame to a CSV file
df.to_csv('data.csv', index=False)

print("CSV file saved in Google Drive.")

"""Link to data.csv file

Official w3schools:
https://www.w3schools.com/python/data.csv

Alternate:
https://www.w3schools.com/python/data.csv

### Create and display a decision tree
"""

df = pd.read_csv("data.csv")

d = {'UK': 0, 'USA': 1, 'N': 2}
df['Nationality'] = df['Nationality'].map(d)
d = {'YES': 1, 'NO': 0}
df['Go'] = df['Go'].map(d)

features = ['Age', 'Experience', 'Rank', 'Nationality']

X = df[features]
y = df['Go']

dtree = DecisionTreeClassifier()
dtree = dtree.fit(X, y)

tree.plot_tree(dtree, feature_names=features)

"""## ----------------------------------------------------------------------------

## Confusion Matrix

Explains the metrics: Accuracy, Precision, Sensitivity (Recall), Specificity, and the F-score

Uses `metrics.confusion_matrix()` to build the matrix and `metrics.ConfusionMatrixDisplay()` to display it

### Theory

**What is a confusion matrix?**

It is a table that is used in classification problems to assess where errors in the model were made.

The rows represent the actual classes the outcomes should have been. While the columns represent the predictions we have made. Using this table it is easy to see which predictions are wrong.

**Four different quadrants of The Confusion Matrix**

* True Negative (Top-Left Quadrant)
* False Positive (Top-Right Quadrant)
* False Negative (Bottom-Left Quadrant)
* True Positive (Bottom-Right Quadrant)

True means that the values were accurately predicted, False means that there was an error or wrong prediction.

**Created Metrics**

* Accuracy
* Precision
* Sensitivity (Recall)
* Specificity
* F-score

**Accuracy**

Accuracy measures how often the model is correct.

How to Calculate:

`(True Positive + True Negative) / Total Predictions`

**Precision**

Of the positives predicted, what percentage is truly positive?

How to Calculate:

`True Positive / (True Positive + False Positive)`

Precision does not evaluate the correctly predicted negative cases:

**Sensitivity (Recall)**

Of all the positive cases, what percentage are predicted positive?

Sensitivity (sometimes called Recall) measures how good the model is at predicting positives.

This means it looks at true positives and false negatives (which are positives that have been incorrectly predicted as negative).

How to Calculate:

`True Positive / (True Positive + False Negative)`

Sensitivity is good at understanding how well the model predicts something is positive:

**Specificity**

How well the model is at prediciting negative results?

Specificity is similar to sensitivity, but looks at it from the persepctive of negative results.

How to Calculate:

`True Negative / (True Negative + False Positive)`

Since it is just the opposite of Recall, we use the recall_score function, taking the opposite position label:

**F-score**

F-score is the "harmonic mean" of precision and sensitivity.

It considers both false positive and false negative cases and is good for imbalanced datasets.

How to Calculate:

`2 * ((Precision * Sensitivity) / (Precision + Sensitivity))`

This score does not take into consideration the True Negative values:

### Import Dependencies

(If not done so already)
"""

import numpy as np
from sklearn import metrics
import matplotlib.pyplot as plt

"""In the following example, we have:
The x array represents the age of each car.
The y array represents the speed of each car.
```
x = [5,7,8,7,2,17,2,9,4,11,12,9,6]

y = [99,86,87,88,111,86,103,87,94,78,77,85,86]
```

We will be performing actions on this data set

### Creating a Confusion Matrix
"""

actual = np.random.binomial(1,.9,size = 1000)
predicted = np.random.binomial(1,.9,size = 1000)

confusion_matrix = metrics.confusion_matrix(actual, predicted)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])

cm_display.plot()
plt.show()

"""### Explaining the code

**Data Generation:**

Generates 1000 random samples from a binomial distribution with a probability of success (1) set to 0.9. This simulates the actual outcomes.

```
actual = numpy.random.binomial(1,.9,size = 1000)
```

Generates 1000 random samples from a binomial distribution with a probability of success (1) also set to 0.9. This simulates the predicted outcomes from a model.

```
predicted = numpy.random.binomial(1,.9,size = 1000)
```

**Confusion Matrix Calculation:**

Calculates the confusion matrix using the actual and predicted values. The confusion matrix is a table that is often used to describe the performance of a classification model.

```
confusion_matrix = metrics.confusion_matrix(actual, predicted)
```

**Confusion Matrix Display:**

Creates a ConfusionMatrixDisplay object, which is a wrapper around the confusion matrix with options for display.

```
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[0, 1])
```

Plots the confusion matrix using the plot() method.
```
cm_display.plot()
```

**Visualization**

Displays the confusion matrix plot.

```
plt.show()
```

### Testing different matrics

**Accuracy**
"""

Accuracy = metrics.accuracy_score(actual, predicted)
print(Accuracy)

"""**Precision**"""

Precision = metrics.precision_score(actual, predicted)
print(Precision)

"""**Sensitivity (Recall)**"""

Sensitivity_recall = metrics.recall_score(actual, predicted)
print(Sensitivity_recall)

"""**Specificity**"""

Specificity = metrics.recall_score(actual, predicted, pos_label=0)
print(Specificity)

"""**F-score**"""

F1_score = metrics.f1_score(actual, predicted)
print(F1_score)

"""## ----------------------------------------------------------------------------

## Hierarchical Clustering

Groups data into clusters based on rules

Here, *euclidean distance* and *the Ward linkage method* were expined

*Dendrogram* was also used to plot `dendrogram(linkage_data)`

The data was clustered in the following ways

```
linkage_data = linkage(data, method='ward', metric='euclidean')
```

and

```
hierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
labels = hierarchical_cluster.fit_predict(data)
```

### Import Dependencies

(If not done so already)
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

"""In the following example, we have:
The x array represents the age of each car.
The y array represents the speed of each car.
```
x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]
```

We will be performing actions on this data set

### Theory

**What is Hierarchical clustering?**

Hierarchical clustering organizes data points into clusters based on their similarities. It doesn't require training data or a target variable. It's useful for visualizing relationships between data points. In this case, we'll use it to group data points and display clusters with a dendrogram and scatter plot.

**How does it work?**

We will use Agglomerative Clustering, a type of hierarchical clustering that follows a bottom up approach.

1. We begin by treating each data point as its own cluster.

2. Then, we join clusters together that have the shortest distance between them to create larger clusters.

3. This step is repeated until one large cluster is formed containing all of the data points.

Hierarchical clustering requires us to decide on both a distance and linkage method. We will use euclidean distance and the Ward linkage method, which attempts to minimize the variance between clusters.

`Euclidean distance:`
* It's calculated as the square root of the sum of the squared differences between corresponding coordinates in higher dimentions
* In two dimensions, it's the length of the line segment connecting two points.

`Ward linkage method:`
* A criterion used in hierarchical clustering to decide how to merge clusters at each step.
* It aims to minimize the variance when forming clusters by merging the two clusters that result in the smallest increase in total within-cluster variance.
* (In other words, it seeks to minimize the variance within each cluster while maximizing the variance between clusters)
* This method is often preferred when the goal is to create compact, spherical clusters of roughly equal sizes.

`Dendrogram:`
* A dendrogram is a tree-like diagram used in hierarchical clustering to visualize the arrangement of clusters and the relationships between them. It illustrates the merging process of clusters as the algorithm progresses.

Here's how it works:

* Each data point starts as its own cluster.
* At each step, the two closest clusters are merged into a single cluster.
* The dendrogram represents these mergings through the height at which the clusters are joined on the vertical axis.
* The height of each merge indicates the similarity (or distance) between clusters: the lower the merge, the more similar the clusters being merged.

### Compute the ward linkage using euclidean distance, and visualize it using a dendrogram
"""

x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]

data = list(zip(x, y))

linkage_data = linkage(data, method='ward', metric='euclidean')
dendrogram(linkage_data)

plt.show()

"""### Code explained

**Turn the data into a set of points:**

```
data = list(zip(x, y))
```

**Compute the linkage between all of the different points**

Here we use a simple euclidean distance measure and Ward's linkage, which seeks to minimize the variance between clusters.

```
linkage_data = linkage(data, method='ward', metric='euclidean')
```

**Plot the results in a dendrogram**

This plot will show us the hierarchy of clusters from the bottom (individual points) to the top (a single cluster consisting of all data points).

```
dendrogram(linkage_data)
plt.show()
```

### Do the same thing with Python's scikit-learn library. Then, visualize on a 2-dimensional plot
"""

x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]

data = list(zip(x, y))

hierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
labels = hierarchical_cluster.fit_predict(data)

plt.scatter(x, y, c=labels)
plt.show()

"""### Code explained

The scikit-learn library allows us to use hierarchichal clustering in a different manner. First, we initialize the `AgglomerativeClustering` class with 2 clusters, using the same euclidean distance and Ward linkage.

```
hierarchical_cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
```

The `.fit_predict` method can be called on our data to compute the clusters using the defined parameters across our chosen number of clusters.

```
labels = hierarchical_cluster.fit_predict(data)
```

Plot the data

```
plt.scatter(x, y, c=labels)
plt.show()
```

## ----------------------------------------------------------------------------

## Logistic Regression

Logistic regression gives probability in log. Converting log-odds (from `log_odds = logr.coef_`) to probability was explained


Logistic Regression performed with
```
logr = linear_model.LogisticRegression()
logr.fit(X,y)
```

### Theory

Logistic regression aims to solve classification problems. It does this by predicting categorical outcomes, unlike linear regression that predicts a continuous outcome.

In the simplest case there are two outcomes, which is called binomial

A common example for multinomial logistic regression would be predicting the class of an iris flower between 3 different species.

**Coefficient**

In logistic regression the coefficient is the expected change in log-odds of having the outcome per unit change in X.

### Import Dependencies

(If not done so already)
"""

import numpy as np
from sklearn import linear_model

"""In the following example, we have:

`X` represents the size of a tumor in centimeters

`y` represents whether or not the tumor is cancerous (0 for "No", 1 for "Yes")

```
X = np.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)
y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])
```

We will be performing actions on this data set

### Predicting with binomial logistic regression
"""

#Reshaped for Logistic function.
X = np.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)
y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])

logr = linear_model.LogisticRegression()
logr.fit(X,y)

log_odds = logr.coef_
odds = np.exp(log_odds)
print(odds)

#predict if tumor is cancerous where the size is 3.46mm:
predicted = logr.predict(np.array([3.46]).reshape(-1,1))
print(predicted)

"""### Code explained

**Preparing data for LogisticRegression() function**

```
X = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)
y = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])
```

**Using Logistic Regression**

```
logr = linear_model.LogisticRegression()
logr.fit(X,y)
```

**Coefficient**

This tells us that as the size of a tumor increases by 1mm the odds of it being a cancerous tumor increases by 4x.

```
log_odds = logr.coef_
odds = numpy.exp(log_odds)
print(odds)
```
Output: `[4.03541657]`

**Prediction**

We have predicted that a tumor with a size of 3.46mm will not be cancerous.

```
predicted = logr.predict(np.array([3.46]).reshape(-1,1))
print(predicted)
```
Output: `[0]`

### Probability: Create a function that uses the model's coefficient and intercept values to return a new value

The coefficient and intercept values can be used to find the probability that each tumor is cancerous.
"""

def logit2prob(logr,x):
  log_odds = logr.coef_ * x + logr.intercept_
  odds = np.exp(log_odds)
  probability = odds / (1 + odds)
  return(probability)

X = np.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)
y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])

logr = linear_model.LogisticRegression()
logr.fit(X,y)

def logit2prob(logr, X):
  log_odds = logr.coef_ * X + logr.intercept_
  odds = np.exp(log_odds)
  probability = odds / (1 + odds)
  return(probability)

print(logit2prob(logr, X))

# 3.78 0.61 The probability that a tumor with the size 3.78cm is cancerous is 61%.
# 2.44 0.19 The probability that a tumor with the size 2.44cm is cancerous is 19%.
# 2.09 0.13 The probability that a tumor with the size 2.09cm is cancerous is 13%.

"""### Code explained

To find the log-odds for each observation, we must first create a formula that looks similar to the one from linear regression, extracting the coefficient and the intercept.

To then convert the log-odds to odds we must exponentiate the log-odds.

Now that we have the odds, we can convert it to probability by dividing it by 1 plus the odds.

**Finding Log-Odds:**

```
log_odds = logr.coef_ * x + logr.intercept_
```

**Converting Log-Odds to Odds:**

```
odds = np.exp(log_odds)
```

**Converting Odds to Probability:**

```
probability = odds / (1 + odds)
```

## ----------------------------------------------------------------------------

## Grid Search

Try out different values of `C` and then pick the value that gives the best score

### Import Dependencies

(If not done so already)
"""

from sklearn.linear_model import LogisticRegression
from sklearn import datasets

"""In the following example, we have:
The x array represents the age of each car.
The y array represents the speed of each car.
```
x = [5,7,8,7,2,17,2,9,4,11,12,9,6]

y = [99,86,87,88,111,86,103,87,94,78,77,85,86]
```

We will be performing actions on this data set

### Theory

The majority of machine learning models contain parameters that can be adjusted to vary how the model learns. For example, the logistic regression model, from `sklearn`, has a parameter `C` that controls regularization,which affects the complexity of the model.

How do we pick the best value for C? The best value is dependent on the data used to train the model.

One method is to try out different values and then pick the value that gives the best score. This technique is known as a grid search. If we had to select the values for two or more parameters, we would evaluate all combinations of the sets of values thus forming a grid of values.

`Note:`

Higher values of C tell the model, the training data resembles real world information, place a greater weight on the training data. While lower values of C do the opposite.

### Results we can generate without a grid search using only the base parameters

With the default setting of C = 1, we achieved a score of 0.973.
"""

from sklearn import datasets
from sklearn.linear_model import LogisticRegression

iris = datasets.load_iris()

X = iris['data']
y = iris['target']

logit = LogisticRegression(max_iter = 10000)

print(logit.fit(X,y))

print(logit.score(X,y))

"""### Code explained

We load a dataset from `sklearn` as `iris` and just perform `LogisticRegression()` on it

### Implementing Grid Search

Since the default value for C is 1, we will set a range of values surrounding it.

```
C = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]
```
"""

from sklearn import datasets
from sklearn.linear_model import LogisticRegression

iris = datasets.load_iris()

X = iris['data']
y = iris['target']

logit = LogisticRegression(max_iter = 10000)

C = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]

scores = []

for choice in C:
  logit.set_params(C=choice)
  logit.fit(X, y)
  scores.append(logit.score(X, y))

print(scores)

"""We can see that the lower values of C performed worse than the base parameter of 1. However, as we increased the value of C to 1.75 the model experienced increased accuracy.

It seems that increasing C beyond this amount does not help increase model accuracy.

### Code explained

Since the default value for C is 1, we will set a range of values surrounding it.

```
C = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]
```

Next we will create a for loop to change out the values of C and evaluate the model with each change.

```
scores = []

for choice in C:
  logit.set_params(C=choice)
  logit.fit(X, y)
  scores.append(logit.score(X, y))
```

With the scores stored in a list, we can evaluate what the best choice of C is.

```
print(scores)
```

## ----------------------------------------------------------------------------

## Categorical Data

When your data has categories represented by strings, you can tranform the data so it can be used in your models.

We do this with a `0 or 1` for the string's column as follows:

```
ohe_cars = pd.get_dummies(cars[['Car']])

X = pd.concat([cars[['Volume', 'Weight']], ohe_cars], axis=1)

```

### Theory

`What is the need for categorical data?`

When your data has categories represented by strings, you can tranform the data so it can be used in your models.

A linear relationship between a categorical variable, Car or Model, and a numeric variable, CO2, cannot be determined.

`How to perform hot encoding?`

We must have a numeric representation of the categorical variable. One way to do this is to have a column representing each group in the category.

For each column, the values will be 1 or 0 where 1 represents the inclusion of the group and 0 represents the exclusion. This transformation is called one hot encoding.

**Note:**

You do not have to do this manually, the Python Pandas module has a function that called get_dummies() which does one hot encoding.

### Import Dependencies

(If not done so already)
"""

import pandas as pd
from sklearn import linear_model

"""In the following example, we have the same data set that we used in the multiple regression chapter.

data.csv
```
Car,Model,Volume,Weight,CO2
Toyoty,Aygo,1000,790,99
Mitsubishi,Space Star,1200,1160,95
Skoda,Citigo,1000,929,95
Fiat,500,900,865,90
Mini,Cooper,1500,1140,105
VW,Up!,1000,929,105
Skoda,Fabia,1400,1109,90
Mercedes,A-Class,1500,1365,92
Ford,Fiesta,1500,1112,98
Audi,A1,1600,1150,99
Hyundai,I20,1100,980,99
Suzuki,Swift,1300,990,101
Ford,Fiesta,1000,1112,99
Honda,Civic,1600,1252,94
Hundai,I30,1600,1326,97
Opel,Astra,1600,1330,97
BMW,1,1600,1365,99
Mazda,3,2200,1280,104
Skoda,Rapid,1600,1119,104
Ford,Focus,2000,1328,105
Ford,Mondeo,1600,1584,94
Opel,Insignia,2000,1428,99
Mercedes,C-Class,2100,1365,99
Skoda,Octavia,1600,1415,99
Volvo,S60,2000,1415,99
Mercedes,CLA,1500,1465,102
Audi,A4,2000,1490,104
Audi,A6,2000,1725,114
Volvo,V70,1600,1523,109
BMW,5,2000,1705,114
Mercedes,E-Class,2100,1605,115
Volvo,XC70,2000,1746,117
Ford,B-Max,1600,1235,104
BMW,216,1600,1390,108
Opel,Zafira,1600,1405,109
Mercedes,SLK,2500,1395,120

```

We will be performing actions on this data set

### Download csv file
"""

import io
import os

data = """
Car,Model,Volume,Weight,CO2
Toyoty,Aygo,1000,790,99
Mitsubishi,Space Star,1200,1160,95
Skoda,Citigo,1000,929,95
Fiat,500,900,865,90
Mini,Cooper,1500,1140,105
VW,Up!,1000,929,105
Skoda,Fabia,1400,1109,90
Mercedes,A-Class,1500,1365,92
Ford,Fiesta,1500,1112,98
Audi,A1,1600,1150,99
Hyundai,I20,1100,980,99
Suzuki,Swift,1300,990,101
Ford,Fiesta,1000,1112,99
Honda,Civic,1600,1252,94
Hundai,I30,1600,1326,97
Opel,Astra,1600,1330,97
BMW,1,1600,1365,99
Mazda,3,2200,1280,104
Skoda,Rapid,1600,1119,104
Ford,Focus,2000,1328,105
Ford,Mondeo,1600,1584,94
Opel,Insignia,2000,1428,99
Mercedes,C-Class,2100,1365,99
Skoda,Octavia,1600,1415,99
Volvo,S60,2000,1415,99
Mercedes,CLA,1500,1465,102
Audi,A4,2000,1490,104
Audi,A6,2000,1725,114
Volvo,V70,1600,1523,109
BMW,5,2000,1705,114
Mercedes,E-Class,2100,1605,115
Volvo,XC70,2000,1746,117
Ford,B-Max,1600,1235,104
BMW,216,1600,1390,108
Opel,Zafira,1600,1405,109
Mercedes,SLK,2500,1395,120
"""

if os.path.exists('data.csv'):
    os.remove('data.csv')

# Convert the raw data to a DataFrame
df = pd.read_csv(io.StringIO(data))

# Write DataFrame to a CSV file
df.to_csv('data.csv', index=False)

print("CSV file saved in Google Drive.")

"""Link to data.csv file

Official w3schools:
https://www.w3schools.com/python/data.csv

Alternate:
https://www.w3schools.com/python/data.csv

### Predict the CO2 emissions based on the car's weight, volume, and manufacturer.
"""

cars = pd.read_csv("data.csv")
ohe_cars = pd.get_dummies(cars[['Car']])

X = pd.concat([cars[['Volume', 'Weight']], ohe_cars], axis=1)
y = cars['CO2']

regr = linear_model.LinearRegression()
regr.fit(X,y)

##predict the CO2 emission of a Volvo where the weight is 2300kg, and the volume is 1300cm3:
predictedCO2 = regr.predict([[2300, 1300,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]])

print(predictedCO2)

"""### Code explained

We create the dummy variables for categorical data

```
ohe_cars = pd.get_dummies(cars[['Car']])
```

Then we must select the independent variables (`X`) and add the dummy variables columnwise. Also store the dependent variable in `y`.

```
X = pd.concat([cars[['Volume', 'Weight']], ohe_cars], axis=1)
y = cars['CO2']
```

## ----------------------------------------------------------------------------

## K-means

Groups data into `K` clusters

Does this using:
```
kmeans = KMeans(n_clusters=2)
kmeans.fit(data)
```

We have to loop to find best value of K. Best would be when there is a huge improvement. We do this by checking `inertia` as follows

```
inertias =[]
inertias.append(kmeans.inertia_)
```

### Theory

`What is K-Means?`

K-means is an unsupervised learning method for clustering data points. The algorithm iteratively divides data points into K clusters by minimizing the variance in each cluster.

`How do we estimate the best K value?`

Here, we will estimate the best value for K using the elbow method, then use K-means clustering to group the data points into clusters.

`How does it work?`

First, each data point is randomly assigned to one of the K clusters. Then, we compute the centroid (functionally the center) of each cluster, and reassign each data point to the cluster with the closest centroid. We repeat this process until the cluster assignments for each data point are no longer changing

.

K-means clustering requires us to select K, the number of clusters we want to group the data into. The elbow method lets us graph the inertia (a distance-based metric) and visualize the point at which it starts decreasing linearly. This point is referred to as the "eblow" and is a good estimate for the best value for K based on our data.

### Import Dependencies

(If not done so already)
"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

"""In the following example, we have:
The x array represents the age of each car.
The y array represents the speed of each car.
```
x = [5,7,8,7,2,17,2,9,4,11,12,9,6]

y = [99,86,87,88,111,86,103,87,94,78,77,85,86]
```

We will be performing actions on this data set

### Utilize the elbow method to visualize the intertia for different values of K
"""

x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]

data = list(zip(x, y))
inertias = []

for i in range(1,11):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(data)
    inertias.append(kmeans.inertia_)

plt.plot(range(1,11), inertias, marker='o')
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

"""The elbow method shows that 2 is a good value for K, so we retrain and visualize the result:


"""

x = [4, 5, 10, 4, 3, 11, 14 , 6, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]

data = list(zip(x, y))

kmeans = KMeans(n_clusters=2)
kmeans.fit(data)

plt.scatter(x, y, c=kmeans.labels_)
plt.show()

"""### Code explained

Turn the data into a set of points:

```
data = list(zip(x, y))
print(data)
```

Result:
`[(4, 21), (5, 19), (10, 24), (4, 17), (3, 16), (11, 25), (14, 24), (6, 22), (10, 21), (12, 21)]`

In order to find the best value for K, we need to run K-means across our data for a range of possible values. We only have 10 data points, so the maximum number of clusters is 10. So for each value K in range(1,11)

```
for i in range(1,11):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(data)
    inertias.append(kmeans.inertia_)
```

Plotting

```
plt.plot(range(1,11), inertias, marker='o')
plt.title('Elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()
```

## ----------------------------------------------------------------------------

## Bootstrap Aggregation (Bagging)

It attempts to resolve overfitting for classification or regression problems with the aim to improve the accuracy and performance

Here, we see how the best `n_estimators` is guessed in `BaggingClassifier()` for a decision tree

### Import Dependencies

(If not done so already)
"""

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

"""### Theory

`What is Bootstrap Aggregation (bagging)?`

Bootstrap Aggregation (bagging) is a ensembling method that attempts to resolve overfitting for classification or regression problems. Bagging aims to improve the accuracy and performance of machine learning algorithms.

`How does bagging improve accuracy and performance?`

It does this by taking random subsets of an original dataset, with replacement, and fits either a classifier (for classification) or regressor (for regression) to each subset. The predictions for each subset are then aggregated through majority vote for classification or averaging for regression, increasing prediction accuracy.

### Evaluating a Base Classifier
"""

# The parameter `as_frame` is set equal to True so
# we do not lose the feature names when loading the data.

data = datasets.load_wine(as_frame = True)

X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 22)

dtree = DecisionTreeClassifier(random_state = 22)
dtree.fit(X_train,y_train)

y_pred = dtree.predict(X_test)

print("Train data accuracy:",accuracy_score(y_true = y_train, y_pred = dtree.predict(X_train)))
print("Test data accuracy:",accuracy_score(y_true = y_test, y_pred = y_pred))

"""### Code explained

Split `X` and `y` into train and test sets

```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 22)
```

Note:

`random_state` is a parameter in train_test_split that controls the random number generator used to shuffle the data before splitting it.

Fit decision tree

```
dtree = DecisionTreeClassifier(random_state = 22)
dtree.fit(X_train,y_train)
```

Predict from decision tree

```
y_pred = dtree.predict(X_test)
```

Result:

```
Train data accuracy: 1.0
Test data accuracy: 0.8222222222222222
```

### Creating a Bagging Classifier
"""

data = datasets.load_wine(as_frame = True)

X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 22)

estimator_range = [2,4,6,8,10,12,14,16]

models = []
scores = []

for n_estimators in estimator_range:

    # Create bagging classifier
    clf = BaggingClassifier(n_estimators = n_estimators, random_state = 22)

    # Fit the model
    clf.fit(X_train, y_train)

    # Append the model and score to their respective list
    models.append(clf)
    scores.append(accuracy_score(y_true = y_test, y_pred = clf.predict(X_test)))

# Generate the plot of scores against number of estimators
plt.figure(figsize=(9,6))
plt.plot(estimator_range, scores)

# Adjust labels and font (to make visable)
plt.xlabel("n_estimators", fontsize = 18)
plt.ylabel("score", fontsize = 18)
plt.tick_params(labelsize = 16)

# Visualize plot
plt.show()

"""### Code explained

Create a range of values that represent the number of estimators we want to use in each ensemble.

```
estimator_range = [2,4,6,8,10,12,14,16]
```

To see how the Bagging Classifier performs with differing values of n_estimators, create a for loop storing the models and scores in separate lists for later vizualizations.

`Note:`

The default parameter for the base classifier in BaggingClassifier is the DicisionTreeClassifier therefore we do not need to set it when instantiating the bagging model.

```
models = []
scores = []

for n_estimators in estimator_range:

    # Create bagging classifier and fit the model
    clf = BaggingClassifier(n_estimators = n_estimators, random_state = 22)
    clf.fit(X_train, y_train)

    # Append the model and score to their respective list
    models.append(clf)
    scores.append(accuracy_score(y_true = y_test, y_pred = clf.predict(X_test)))
```

With the models and scores stored, we can now visualize the improvement in model performance.

```
# Generate the plot of scores against number of estimators
plt.figure(figsize=(9,6))
plt.plot(estimator_range, scores)

# Adjust labels and font (to make visable)
plt.xlabel("n_estimators", fontsize = 18)
plt.ylabel("score", fontsize = 18)
plt.tick_params(labelsize = 16)

# Visualize plot
plt.show()
```

### Generate Decision Trees from Bagging Classifier
"""

X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 22)

clf = BaggingClassifier(n_estimators = 12, oob_score = True,random_state = 22)

clf.fit(X_train, y_train)

plt.figure(figsize=(30, 20))

plot_tree(clf.estimators_[0], feature_names = X.columns)

"""### Create a model with out-of-bag metric.

As bootstrapping chooses random subsets of observations to create classifiers, there are observations that are left out in the selection process. These "out-of-bag" observations can then be used to evaluate the model, similarly to that of a test set. Keep in mind, that out-of-bag estimation can overestimate error in binary classification problems and should only be used as a compliment to other metrics.
"""

data = datasets.load_wine(as_frame = True)

X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 22)

oob_model = BaggingClassifier(n_estimators = 12, oob_score = True,random_state = 22)

oob_model.fit(X_train, y_train)

print(oob_model.oob_score_)

"""`Note:`

We saw in the last exercise that 12 estimators yielded the highest accuracy, so we will use that to create our model. This time setting the parameter oob_score to true to evaluate the model with out-of-bag score.


Since the samples used in OOB and the test set are different, and the dataset is relatively small, there is a difference in the accuracy. It is rare that they would be exactly the same, again OOB should be used quick means for estimating error, but is not the only evaluation metric.

`Results Explained:`

By iterating through different values for the number of estimators we can see an increase in model performance from 82.2% to 95.5%. After 14 estimators the accuracy begins to drop, again if you set a different random_state the values you see will vary. That is why it is best practice to use cross validation to ensure stable results.

.

In this case, we see a 13.3% increase in accuracy when it comes to identifying the type of the wine.

## ----------------------------------------------------------------------------

## Cross Validation

Learnt about `K-Fold`, `Stratified K-Fold`, `Leave-One-Out (LOO)`, `Leave-P-Out (LPO)`, `Shuffle Split`

```
k_folds = KFold(n_splits = 5)
sk_folds = StratifiedKFold(n_splits = 5)
loo = LeaveOneOut()
lpo = LeavePOut(p=2)
ss = ShuffleSplit(train_size=0.6, test_size=0.3, n_splits = 5)
```

### Import Dependencies

(If not done so already)
"""

from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import ShuffleSplit, LeavePOut, LeaveOneOut, StratifiedKFold, KFold, cross_val_score

"""### Theory

`What is cross validation?`

Cross-validation is a technique used to assess how well a model generalizes to unseen data while avoiding information leakage. Instead of relying on a single train-test split, cross-validation involves partitioning the dataset into multiple subsets, training the model on different combinations of these subsets, and then evaluating its performance on the remaining data. This helps to ensure that the model's performance estimates are more robust and less influenced by the specific characteristics of any one test set.

`Why perform cross validation?`

When adjusting models we are aiming to increase overall model performance on unseen data. Hyperparameter tuning can lead to much better performance on test sets. However, optimizing parameters to the test set can lead information leakage causing the model to preform worse on unseen data. To correct for this we can perform cross validation.

`Hyperparameters:`

Hyperparameters are settings or configurations that are not learned from the data but are set prior to training the model.

Examples include the learning rate in neural networks or the depth of a decision tree

Hyperparameter tuning involves experimenting with different values for these settings to find the combination that produces the best performance on a validation dataset. Proper tuning can significantly improve a model's performance on unseen data.

### Different methods to cross validate

`clf = DecisionTreeClassifier(random_state=42)`

**K-Fold**

The training data used in the model is split, into k number of smaller sets, to be used to validate the model. The model is then trained on k-1 folds of training set.


```
k_folds = KFold(n_splits = 5)

scores = cross_val_score(clf, X, y, cv = k_folds)
```

**Stratified K-Fold**

In cases where classes are imbalanced we need a way to account for the imbalance in both the train and validation sets. To do so we can stratify the target classes, meaning that both sets will have an equal proportion of all classes.

`Note:`

Classes typically refer to the different categories or labels that the data points belong to.

```
sk_folds = StratifiedKFold(n_splits = 5)

scores = cross_val_score(clf, X, y, cv = sk_folds)
```

**Leave-One-Out (LOO)**

Instead of selecting the number of splits in the training data set like k-fold LeaveOneOut, utilize 1 observation to validate and n-1 observations to train. This method is an exaustive technique.

```
loo = LeaveOneOut()

scores = cross_val_score(clf, X, y, cv = loo)
```

**Leave-P-Out (LPO)**

Leave-P-Out is simply a nuanced diffence to the Leave-One-Out idea, in that we can select the number of p to use in our validation set.

```
lpo = LeavePOut(p=2)

scores = cross_val_score(clf, X, y, cv = lpo)
```

**Shuffle Split**

Unlike `KFold`, `ShuffleSplit` leaves out a percentage of the data, not to be used in the train or validation sets. To do so we must decide what the train and test sizes are, as well as the number of splits.

```
ss = ShuffleSplit(train_size=0.6, test_size=0.3, n_splits = 5)

scores = cross_val_score(clf, X, y, cv = ss)
```

Example, run k-fold CV
"""

X, y = datasets.load_iris(return_X_y=True)

clf = DecisionTreeClassifier(random_state=42)

# Switch this with other things
k_folds = KFold(n_splits = 5)

scores = cross_val_score(clf, X, y, cv = k_folds)

print("Cross Validation Scores: ", scores)
print("Average CV Score: ", scores.mean())
print("Number of CV Scores used in Average: ", len(scores))

"""## ----------------------------------------------------------------------------

## AUC - ROC Curve

### Import Dependencies

(If not done so already)
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve

"""### Theory

**Different evaluation metrics**

Most popular is accuracy but you might consider using another evaluation metric such as AUC

`AUC`

Area under the receiver operating characteristic (ROC) curve

The Reciever operating characteristic curve plots the true positive (TP) rate versus the false positive (FP) rate at different classification thresholds.

The thresholds are different probability cutoffs that separate the two classes in binary classification.

It uses probability to tell us how well a model separates the classes.

**AUC Score**

An AUC score of around .5 would mean that the model is unable to make a distinction between the two classes and the curve would look like a line with a slope of 1.

An AUC score closer to 1 means that the model has the ability to separate the two classes and the curve would come closer to the top left corner of the graph.

**AUC score importance**

Because AUC is a metric that utilizes probabilities of the class predictions, we can be more confident in a model that has a higher AUC score than one with a lower score even if they have similar accuracies.

Even when the accuracies for the two models are similar, the model with the higher AUC score will be more reliable because it takes into account the predicted probability. It is more likely to give you higher accuracy when predicting future data.

### Imbalanced Data

Suppose we have an imbalanced data set where the majority of our data is of one value. We can obtain high accuracy for the model by predicting the majority class.
"""

n = 10000
ratio = .95
n_0 = int((1-ratio) * n)
n_1 = int(ratio * n)

y = np.array([0] * n_0 + [1] * n_1)
# below are the probabilities obtained from a hypothetical model that always predicts the majority class
# probability of predicting class 1 is going to be 100%
y_proba = np.array([1]*n)
y_pred = y_proba > .5

print(f'accuracy score: {accuracy_score(y, y_pred)}')
cf_mat = confusion_matrix(y, y_pred)
print('Confusion matrix')
print(cf_mat)
print(f'class 0 accuracy: {cf_mat[0][0]/n_0}')
print(f'class 1 accuracy: {cf_mat[1][1]/n_1}')

"""Although we obtain a very high accuracy, the model provided no information about the data so it's not useful.

We accurately predict class 1 100% of the time while inaccurately predict class 0 0% of the time.


At the expense of accuracy, it might be better to have a model that can somewhat separate the two classes.

### Probabilities obtained from a hypothetical model that doesn't always predict the mode
"""

n = 10000
ratio = .95
n_0 = int((1-ratio) * n)
n_1 = int(ratio * n)

y = np.array([0] * n_0 + [1] * n_1)

# below are the probabilities obtained from a hypothetical model that
# doesn't always predict the mode

y_proba_2 = np.array(
    np.random.uniform(0, .7, n_0).tolist() +
    np.random.uniform(.3, 1, n_1).tolist()
)
y_pred_2 = y_proba_2 > .5

print(f'accuracy score: {accuracy_score(y, y_pred_2)}')
cf_mat = confusion_matrix(y, y_pred_2)
print('Confusion matrix')
print(cf_mat)
print(f'class 0 accuracy: {cf_mat[0][0]/n_0}')
print(f'class 1 accuracy: {cf_mat[1][1]/n_1}')

"""For the second set of predictions, we do not have as high of an accuracy score as the first but the accuracy for each class is more balanced.

Using accuracy as an evaluation metric we would rate the first model higher than the second even though it doesn't tell us anything about the data.

### Using the evaluation metric AUC
"""

def plot_roc_curve(true_y, y_prob):
    """
    plots the roc curve based of the probabilities
    """

    fpr, tpr, thresholds = roc_curve(true_y, y_prob)
    plt.plot(fpr, tpr)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')

"""**Model 1**"""

plot_roc_curve(y, y_proba)
print(f'model 1 AUC score: {roc_auc_score(y, y_proba)}')

"""model 1 AUC score: 0.5

**Model 2**
"""

plot_roc_curve(y, y_proba_2)
print(f'model 2 AUC score: {roc_auc_score(y, y_proba_2)}')

"""model 2 AUC score: 0.8270551578947367

## ----------------------------------------------------------------------------

## K-nearest neighbors

### Import Dependencies

(If not done so already)
"""

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

"""### Using K means

Create the data points
"""

x = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]
classes = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]

plt.scatter(x, y, c=classes)
plt.show()

"""Fit a KNN algorithm with K=1

Here, it will just search for the closest point
"""

data = list(zip(x, y))
knn = KNeighborsClassifier(n_neighbors=1)

knn.fit(data, classes)

"""Use it to classify a new data point:


"""

new_x = 8
new_y = 21
new_point = [(new_x, new_y)]

prediction = knn.predict(new_point)

plt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])
plt.text(x=new_x-1.7, y=new_y-0.7, s=f"new point, class: {prediction[0]}")
plt.show()

"""Do the same thing, but with a higher K value (which changes the prediction):

Here, it will check between 5 points with K=5
"""

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(data, classes)

prediction = knn.predict(new_point)

plt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])
plt.text(x=new_x-1.7, y=new_y-0.7, s=f"new point, class: {prediction[0]}")
plt.show()

"""### Code Explained

Create arrays that resemble variables in a dataset. We have two input features (x and y) and then a target class (class).

The input features that are pre-labeled with our target class will be used to predict the class of new data.

Note that while we only use two input features here, this method will work with any number of variables:

```
x = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]
y = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]
classes = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]
```

Turn the input features into a set of points:

```
data = list(zip(x, y))
```

Using the input features and target class, we fit a KNN model on the model using 1 nearest neighbor (Note: `n_neighbors=5` later):

```
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(data, classes)
```

Then, we can use the same KNN object to predict the class of new, unforeseen data points. First we create new x and y features, and then call knn.predict() on the new data point to get a class of 0 or 1:

```
new_x = 8
new_y = 21
new_point = [(new_x, new_y)]
prediction = knn.predict(new_point)
print(prediction)
```

When we plot all the data along with the new point and class, we can see it's been labeled blue with the 1 class. The text annotation is just to highlight the location of the new point:

```
plt.scatter(x + [new_x], y + [new_y], c=classes + [prediction[0]])
plt.text(x=new_x-1.7, y=new_y-0.7, s=f"new point, class: {prediction[0]}")
plt.show()
```

## ----------------------------------------------------------------------------

## Contact

Reach out to me

### Ashutosh Karanam

Email: ashutoshdeveloping@gmail.com

LinkedIn: https://linkedin.com/in/karanamashutosh

GitHub: https://github.com/KarAshutosh/
"""